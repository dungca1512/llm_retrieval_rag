{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562f9b632b1c7f19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T03:36:08.361048Z",
     "start_time": "2024-07-15T03:36:07.803733Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d91bb3dbf786fc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T03:36:11.793513Z",
     "start_time": "2024-07-15T03:36:11.745787Z"
    }
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53810657a8f7f2a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T03:36:31.276268Z",
     "start_time": "2024-07-15T03:36:31.273653Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"../data/papers/2405.13576v1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de3b41617c8ab47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T03:36:36.711734Z",
     "start_time": "2024-07-15T03:36:36.393371Z"
    }
   },
   "outputs": [],
   "source": [
    "text = read_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb47535632811769",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T03:36:39.908258Z",
     "start_time": "2024-07-15T03:36:39.899721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FlashRAG: A Modular Toolkit for Efficient\\nRetrieval-Augmented Generation Research\\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou∗\\nGaoling School of Artificial Intelligence\\nRenmin University of China\\n{jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com\\nAbstract\\nWith the advent of Large Language Models (LLMs), the potential of Retrieval\\nAugmented Generation (RAG) techniques have garnered considerable research\\nattention. Numerous novel algorithms and models have been introduced to enhance\\nvarious aspects of RAG systems. However, the absence of a standardized framework\\nfor implementation, coupled with the inherently intricate RAG process, makes it\\nchallenging and time-consuming for researchers to compare and evaluate these\\napproaches in a consistent environment. Existing RAG toolkits like LangChain\\nand LlamaIndex, while available, are often heavy and unwieldy, failing to\\nmeet the personalized needs of researchers. In response to this challenge, we\\npropose FlashRAG, an efficient and modular open-source toolkit designed to assist\\nresearchers in reproducing existing RAG methods and in developing their own\\nRAG algorithms within a unified framework. Our toolkit implements 12 advanced\\nRAG methods and has gathered and organized 32 benchmark datasets. Our toolkit\\nhas various features, including customizable modular framework, rich collection\\nof pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-\\nprocessing scripts, and extensive and standard evaluation metrics. Our toolkit and\\nresources are available at https://github.com/RUC-NLPIR/FlashRAG.\\n1 Introduction\\nIn the era of large language models (LLMs), retrieval-augmented generation (RAG) [ 1,2] has\\nemerged as a robust solution to mitigate hallucination issues in LLMs by leveraging external\\nknowledge bases [ 3]. The substantial applications and the potential of RAG technology have\\nattracted considerable research attention. With the introduction of a large number of new algorithms\\nand models to improve various facets of RAG systems in recent years, comparing and evaluating\\nthese methods under a consistent setting has become increasingly challenging.\\nMany works are not open-source or have fixed settings in their open-source code, making it difficult\\nto adapt to new data or innovative components. Besides, the datasets and retrieval corpus used often\\nvary, with resources being scattered, which can lead researchers to spend excessive time on pre-\\nprocessing steps instead of focusing on optimizing their methods. Furthermore, due to the complexity\\nof RAG systems, involving multiple steps such as indexing, retrieval, and generation, researchers\\noften need to implement many parts of the system themselves. Although there are some existing RAG\\ntoolkits like LangChain [ 4] and LlamaIndex [ 5], they are typically large and cumbersome, hindering\\nresearchers from implementing customized processes and failing to address the aforementioned issues.\\n∗Corresponding author\\nPreprint. Under review.arXiv:2405.13576v1  [cs.CL]  22 May 2024Thus, a unified, researcher-oriented RAG toolkit is urgently needed to streamline methodological\\ndevelopment and comparative studies.\\nTo address the issue mentioned above, we introduce FlashRAG, an open-source library designed to\\nenable researchers to easily reproduce existing RAG methods and develop their own RAG algorithms.\\nThis library allows researchers to utilize built pipelines to replicate existing work, employ provided\\nRAG components to construct their own RAG processes, or simply use organized datasets and corpora\\nto accelerate their own RAG workflow. Compared to existing RAG toolkits, FlashRAG is more suited\\nfor researchers. To summarize, the key features and capabilities of our FlashRAG library can be\\noutlined in the following four aspects:\\nExtensive and Customizable Modular RAG Framework. To facilitate an easily expandable\\nRAG process, we implemented modular RAG at two levels. At the component level, we offer\\ncomprehensive RAG components, including 13 components across four major categories: judger,\\nretriever, refiner, and generator. These components can be used individually in one’s code or\\ncombined to form a cohesive pipeline. At the pipeline level, after reviewing the current state of\\nRAG development, we implemented 8 common RAG processes. Based on this framework, existing\\nmethods can be easily replicated, and RAG processes can be run and evaluated under different\\nsettings.\\nPre-Implemented advanced RAG algorithms. To our knowledge, the implementation of existing\\nwork provided by FlashRAG is the most extensive. So far, based on our framework, we have\\nimplemented 12 advanced RAG algorithms, such as Self-RAG and FLARE, covering Sequential RAG,\\nConditional RAG, Branching RAG, and Loop RAG categories. These methods have been evaluated\\nunder a unified setting, and a benchmark report is available. With our framework, researchers can\\neasily evaluate these methods under various settings and fairly compare them with their own methods,\\nenhancing overall reproducibility. More methods are planned to be incorporated into our library.\\nComprehensive benchmark datasets. To improve the consistency and reusability of datasets in\\nRAG research, we have compiled 32 common RAG benchmark datasets and preprocessed them\\ninto a unified format. Some of these datasets, such as asqa and wikiasp, have undergone specific\\nadjustments for RAG scenarios to ensure consistency. We have hosted these datasets on the Hugging\\nFace platform for easy access and use.\\nEfficient Helping Scripts for RAG. To minimize the setup time in RAG experiments, we offer\\na comprehensive suite of helping scripts, including downloading and slicing Wikipedia for corpus\\ncreation, building indexes for retrieval, and prepare retrieval results in advance. These steps are\\nimportant for the subsequent process, but they are often tedious and can take up a lot of time. Our user-\\nfriendly scripts are designed to be intuitive, ensuring researchers can easily navigate the preparatory\\nstages of RAG-related research.\\n2 Related work\\nThe RAG process often involves various components and complex preliminary handling (such as\\nconstructing corpus and building indexes). Due to the lack of a dedicated RAG library for research,\\nmost open-source codes tend to use their preferred implementation and entail intricate environment\\nconfigurations. Therefore, it is often time-consuming to run others’ code and difficult to migrate to\\nyour own settings. Simultaneously, the processing and use of datasets and corpus lack standardization,\\nenhancing the challenge of making a fair comparison between oneself and existing methods.\\nIn recent years, numerous open-source toolkits pertaining to RAG have been developed, providing\\nrich RAG components. Langchain [ 4], LlamaIndex [ 5], and Haystack [ 6] are among the widely\\nadopted works. These libraries provide a range of advanced APIs related to LLM, such as vector\\ndatabases and embedding models, which greatly streamline the interaction with LLM and facilitate\\nrunning a RAG process effortlessly. Despite the many advantages, these libraries lack support for\\nresearchers. On one hand, they tend to overlook the implementation of existing works including\\nmethods, widely used retrieval corpus, and benchmark datasets. On the other hand, they are often too\\nhefty and heavily encapsulated, obscuring operational details or necessitating complex document\\nsearches, thereby lacking flexibility for customization.\\nGiven these issues, several specialized toolkits for RAG have been introduced that are lighter and\\nmore customizable. For instance, FastRAG [ 7] optimizes based on Haystack’s api and provides a\\n2Table 1: Comparison with other RAG toolkits. Modular Component refers to whether the toolkit is\\ncomposed of modular components. Automatic Evaluation indicates if the toolkit provides automated\\nevaluation capabilities to assess the performance of various datasets. Corpus Helper shows if the\\ntoolkit offers auxiliary tools for processing corpus, including cleaning and chunking.\\nToolkit Modular Component Automatic Evaluation Corpus Helper # Provided Dataset # Support Work\\nLangchain [4] ✓ ✗ ✓ - 2\\nLlamaIndex [5] ✓ ✓ ✓ - 2\\nHaystack [6] ✓ ✓ ✗ - -\\nFastRAG [7] ✓ ✗ ✗ 2 1\\nLocalRQA [8] ✗ ✓ ✗ 3 -\\nAutoRAG [9] ✓ ✓ ✗ 4 -\\nFlashRAG (ours) ✓ ✓ ✓ 32 12\\nlimited number of support methods and benchmark datasets. LocalRQA [ 8] focuses on the training\\nstage in the RAG process, providing comprehensive scripts for training various components (such\\nas retrievers, generators) that might be involved in the RAG process during research. AutoRAG [ 9]\\nadopts a similar design philosophy to ours, implementing modular RAG process. This library\\nrepresents each component in RAG as a node, and the RAG process is achieved by connecting the\\nnodes. Although AutoRAG encompasses a variety of evaluation metrics and benchmarks, it falls short\\nconcerning the direct implementation of existing works. Therefore, in our library, we have not only\\ndesigned an exhaustive assortment of RAG components to implement a wide array of RAG processes\\nbut also implemented various RAG works so that the effects of existing works under various settings\\ncan be replicated directly with a few lines of code. Furthermore, we offer a wealth of resources,\\nincluding a large number of processed datasets, scripts for obtaining and pre-processing widely-used\\ncorpus, among others, to expedite researchers’ preparation time as much as possible.\\n3 The Toolkit: FlashRAG\\nThe FlashRAG is designed to facilitate RAG-related research for researchers. As depicted in Figure 1,\\nthe overall structure of the FlashRAG toolkit comprises three hierarchical modules: the environment\\nmodule, the component module, and the pipeline module. The environment module is fundamental to\\nthe toolkit, establishing the requisite datasets, hyperparameters, and evaluation metrics necessary for\\nexperimentation. Building upon the environment module, the component module consists of various\\nRAG components, each endowed with its specific role (e.g., retrieval and generation). The pipeline\\nmodule synthesizes an assortment of component modules with the purpose of effectuating a complete\\nRAG process. In this paper, we will introduce the component and pipeline modules. Additional\\ndetails are available in the documentation of our library.\\n3.1 Component Module\\nThe Component Module consolidates all elements involved in the RAG process into a unified\\nframework. Each component is equipped with autonomous functionality, enabling standalone\\napplication. Currently, the Component Module encompasses five main components: Judger, Retriever,\\nReranker, Refiner, and Generator.\\nJudger functions as a preliminary component that assesses whether a query necessitates retrieval.\\nGiven the limited work and models in this domain, we presently offer a judger based on the SKR [ 10]\\nmethod, which determines the necessity of retrieval using a curated set of LLM self-knowledge data.\\nRetriever implementations are extensively covered by our toolkit. For sparse retrieval, we have\\nintegrated the Pyserini library [ 11] to facilitate the BM25 method. For dense retrieval, we provide\\nsupport for various BERT-based embedding models such as DPR [ 12], E5 [ 13] and BGE [ 14].\\nFlashRAG also support models based on the T5 architecture like ANCE [ 15]. We employ FAISS [ 16,\\n17] for vector database computations to ensure retrieval efficiency and utilize the HuggingFace’s\\ndatasets library to enhance corpus loading speed.\\nTo enhance the reusability of retrieval results and accommodate non-open source retrievers, our library\\nsupports the use of pre-retrieved results termed \"retrieval cache\". During each retrieval instance,\\nthe system automatically searches the retrieval cache for relevant results using the current query,\\npresenting them as the return value. Using our retrievers, user can set automatic saving of retrieval\\n3DataData setZoo\\nQuestion \\nAnswering Multiple \\nChoice\\nFact \\nVerificationEntity \\nLinkingDialog \\nGeneration\\nOther \\nDatasetsCorpusZoo\\nWikipedia\\nMS MARCOPre-\\nprocessing\\nScripts\\nBasic \\nComponents\\nEmbedding \\nModelsRetriever\\nT5-Based \\nEncodersBM25 \\nRetrieverReranker\\nCross -\\nEncoderEmbedding \\nModelsEncoder -Decoder \\nGeneratorGenerator\\nvllm Generator FastChat  GeneratorDecoder -Only \\nGenerator\\nJudgerAbstractive \\nRefinerRefiner\\nLLMLingua  RefinerSelectiveContext  \\nRefinerRECOMP \\nRefinerExtractive \\nRefiner\\nSKR JudgerPipelinesSequential \\nPipelineConditional \\nPipelineBranching \\nPipelineIterative \\nPipelineLoop Pipeline … …Environment Config File Parameter Dict Evaluation ModuleFigure 1: An overview of the FlashRAG toolkit.\\ncaches as JSONL files for future use. For non-open source retrievers, user can format the retrieval\\nresults to fit our cache structure for loading.\\nReranker aims at refining the order of results returned by the retriever to enhance retrieval accuracy.\\nCurrently, FlashRAG supports a variety of widely-used Cross-Encoder models, such as the bge-\\nreranker and jina-reranker. In scenarios where embedding models are used for reranking (e.g.,\\nemploying BM25 as the retriever), we also facilitate the use of Bi-Encoder models like E5 as\\nrerankers. In practice, the reranker is integrated into the retriever’s retrieval function via a decorator,\\nenabling seamless combination with any retriever. Users can assemble any retriever and reranker\\nwith just one line of code.\\nRefiner refines the input text for generators to reduce token usage and reduce noise from retrieved\\ndocuments, improving the final RAG responses. Serving as an essential part of the RAG process,\\nvarious studies focus on developing superior refinements. We have reviewed the existing literature\\nand implemented four types of refiners, each performing differently in handling retrieved documents.\\nThe Extractive Refiner employs an embedding model to extract semantic units, like sentences or\\nphrases, from the retrieved text that hold higher semantic similarity with the query. The Abstractive\\nRefiner utilizes a seq2seq model to directly summarize the retrieved text, supporting dedicated models\\nlike RECOMP [ 18], as well as the general summarizer models with similar structures available\\non HuggingFace. Furthermore, we also facilitate the use of LLMLingua [ 19,20] Refiner and\\nSelective-Context [21] Refiner, both perplexity-based refiners.\\nGenerator is the final component in the RAG process, thoroughly covered within our toolkit. In\\nthe generator module, we’ve integrated two leading LLM acceleration libraries, vllm [ 22] and\\nFastChat [ 23], hence, a myriad of mainstream LLMs are supported. Furthermore, we provide the\\nnative interface of the Transformers library [ 24] to enhance robustness. We also support various\\nencoder-decoder models, such as Flan-T5 [ 25]. For these models, we facilitate the use of Fusion in\\nDecoder (FiD) techniques [ 26], further optimizing efficiency when dealing with retrieved documents.\\n3.2 Pipeline Module\\nBuilding on the diverse components outlined earlier, we are able to decouple the algorithmic flow of\\nthe RAG process from the specific implementations of each component, facilitating the assembly\\nof the entire pipeline. The entire pipeline processes the dataset provided by the user, executes the\\ncorresponding RAG process on it, and delivers both the final evaluation outcomes and intermediate\\nresults. In constructing the pipeline, one only needs to consider which components are required for\\nthe entire RAG process and the logic of data flow between these components. Specifically, within\\n4each pipeline, it is necessary to load the required components in the init(.) function and implement\\nthe corresponding logic in the run(.) function according to each component’s interface.\\nTo systematically execute the operational logic of various RAG tasks, we conducted an in-depth survey\\nof RAG-related literature. Drawing on the summaries from the RAG survey [ 27], we categorized all\\nRAG process flows into four types: Sequential, Branching, Conditional, and Loop. So far, we have\\nimplemented 8 different pipelines, covering a range of advancing RAG works.\\nSequential Pipeline implements a linear execution path for the query, formally represented as query\\n-> retriever -> post-retrieval (reranker, refiner) -> generator. Once the user has configured their\\nsettings, the library automatically loads the necessary components along with their corresponding\\nprocess logic.\\nBranching Pipeline executes multiple paths in parallel for a single query (often one path per\\nretrieved document) and merges the results from all paths to form the ultimate output. Currently, our\\nlibrary supports two advancing branching methods: REPLUG pipeline [ 28] and SuRe pipeline [ 29].\\nThe REPLUG pipeline processes each retrieved document in parallel and combines the generation\\nprobabilities from all documents to produce the final answer. The SuRe pipeline generates a candidate\\nanswer from each retrieved document and then ranks all candidate answers. In implementing SuRe,\\nwe adhere to the original paper’s prompt and processing flow to ensure accuracy and comparability\\nof the results.\\nConditional Pipeline utilizes a judger to direct the query into different execution paths based on the\\njudgement outcome. In the current framework, queries deemed in need of retrieval are sent into the\\nnormal sequential process, while the rest bypass retrieval and proceed directly to generation. We offer\\nutility functions to split and merge the input dataset based on the judger’s determination, ensuring that\\nall processing can be conducted in batches, which enhances the efficiency of the pipeline. Moreover,\\nthe conditional pipeline supports integration with various types of pipelines, meaning it can execute\\ndifferent pipelines for queries based on different judger outcomes.\\nLoop Pipeline involves complex interactions between retrieval and generation processes, often\\nencompassing multiple cycles of retrieval and generation. Compared to the previous three types\\nof pipelines, this type offers greater flexibility and improved outcomes. We support four widely\\nrecognized methods, including Iterative [ 30,31], Self-Ask [ 32], Self-RAG [ 33], and FlARE [ 34]. For\\neach of these methods, we support flexible adjustments to the retrievers and generators to test their\\nperformances in different scenarios.\\n3.3 Datasets and Corpus\\n3.3.1 Datasets\\nAs shown in Table 2, we collects and pre-processes 32 benchmark datasets, covering the majority\\nof the datasets utilized in RAG works. We researched and listed the sources of answers in each\\ndataset for reference. For most datasets, the knowledge comes from Wikipedia, underscoring its\\nimportance in RAG tasks. All datasets have been formatted into a unified JSONL structure, typically\\nencapsulating four fields: ID, question, golden answer, and metadata. For multiple-choice datasets\\nlike MMLU [ 35,36] and OpenBookQA [ 37], an additional \"choices\" field is provided as options. We\\nhave hosted the processed datasets on HuggingFace for easy access. Details on dataset processing\\ncan be found in the appendix.\\nBesides the datasets, we offer a variety of dataset filtering tools for user to filter the entire dataset.\\nFor instance, user can choose a certain number of samples from the entire dataset, either randomly or\\nsequentially for evaluation, or select a subset of the dataset through the dataset’s metadata. These\\nmethods are unified within a dataset loading function, which is accessible through a standard interface.\\nUsers are also allowed to implement their own filtering functions.\\n3.3.2 Corpus\\nBesides datasets, the corpus used for retrieval, also known as the knowledge base, is another vital\\npreparation of experiments. In various research works, the following two types of corpus are often\\nused: Wikipedia dump and MS MARCO passage.\\n5Table 2: Summary of datasets. FlashRAG currently includes a variety of datasets of different tasks.\\nThe sample size of each dataset and the knowledge source of the answer are listed as references. \"-\"\\nindicates that the knowledge source is common sense. The ∗symbol represents that the task of this\\ndataset has been modified to fit the RAG scene.\\nTask Dataset Name Knowledge Source # Train # Dev # Test\\nNQ [38] Wiki 79,168 8,757 3,610\\nTriviaQA [39] Wiki & Web 78,785 8,837 11,313\\nPopQA [40] Wiki / / 14,267\\nSQuAD [41] Wiki 87,599 10,570 /\\nMSMARCO-QA [42] Web 808,731 101,093 /\\nNarrativeQA [43] Books, movie scripts 32,747 3,461 10,557\\nWikiQA [44] Wiki 20,360 2,733 6,165\\nWebQuestions [45] Google Freebase 3,778 / 2,032\\nAmbigQA [46, 38] Wiki 10,036 2,002 /\\nSIQA [47] - 33,410 1,954 /\\nCommenseQA [48] - 9,741 1,221 /\\nBoolQ [49] Wiki 9,427 3,270 /\\nPIQA [50] - 16,113 1,838 /QA\\nFermi [51] Wiki 8,000 1,000 1,000\\nHotpotQA [52] Wiki 90,447 7,405 /\\n2WikiMultiHopQA [53] Wiki 15,000 12,576 /\\nMusique [54] Wiki 19,938 2,417 /Multi-Hop QA\\nBamboogle [32] Wiki / / 125\\nASQA [55] Wiki 4,353 948 /Long-Form QAELI5 [56] Reddit 272,634 1,507 /\\nMMLU [35, 36] - 99,842 1,531 14,042\\nTruthfulQA [57] Wiki / 817 /\\nHellaSwag [58] ActivityNet 39,905 10,042 /\\nARC [59] - 3,370 869 3,548Multiple-Choice\\nOpenBookQA [37] - 4,957 500 500\\nAIDA CoNLL-YAGO [60, 61] Wiki & Freebase 18,395 4,784 /Entity-linkingWNED [62, 61] Wiki / 8,995 /\\nT-REx [63, 61] DBPedia 2,284,168 5,000 /Slot fillingZero-shot RE [64, 61] Wiki 147,909 3,724 /\\nFact Verification FEVER [65, 61] Wiki 104,966 10,444 /\\nDialog Generation WOW [66, 61] Wiki 63,734 3,054 /\\nOpen-domain Summarization∗WikiAsp [67] Wiki 300,636 37,046 37,368\\nWikipedia passages: The Wikipedia passages comprises a collection of documents from English\\nWikipedia entries, serving as the knowledge source for many datasets, such as KILT [ 61]. It was first\\nintroduced as a retrieval corpus in DrQA [ 68], and subsequently utilized in previous works [ 12,1,34].\\nAcquiring the Wikipedia dump involves a complex process, including downloading Wikipedia\\nsnapshots in XML format, cleaning the text to remove redundant HTML tags and extracting the\\ncorresponding textual content, and segmenting the entire document text into individual passages for\\nretrieval.\\nFor various reasons, there are several different versions of Wikipedia used in existing work, increasing\\nthe difficulty of reproduction. To address this, we provide easy-to-use scripts for automatically\\ndownloading and pre-processing any required Wikipedia version. Additionally, we offer various\\nchunking functions to support custom segmentation methods, enabling researchers to align their\\ncorpus with others’ works or to establish a standard corpus for use. We also provide the widely\\nutilized Wikipedia dump presented by DPR from December 20, 2018, as a fundamental resource.\\nMS MARCO passages [ 42]:The MS MARCO passage includes 8.8 million passages, sourced\\nfrom Bing search engine retrievals. Compared to the Wikipedia dump, it contains fewer passages.\\nFortunately, this corpus has undergone pre-processing, allowing for its direct use. Since this dataset\\nis already hosted on Hugging Face and matches our required format, we provide its original link for\\nease of download.\\n63.4 Evaluation\\nOur library supports a variety of evaluation metrics to assess the quality of RAG process. Depending\\non the subject of evaluation, our supporting metrics can be divided into two categories: retrieval-aspect\\nmetrics and generation-aspect metrics.\\nRetrieval-aspect metrics: To evaluate the quality of the retrieval, we support four metrics including\\nrecall@k, precision@k, F1@k, and mean average precision (MAP). Unlike assessing standalone\\nretrieval systems, the documents retrieved in the RAG process often lack golden labels (e.g., related\\nor unrelated tags). Therefore, we facilitate these evaluations by considering whether the golden\\nanswer is present within the retrieved documents as an indicator of relevance. Other types of metrics\\ncan be obtained by inheriting existing metrics and modifying the calculation methods inside.\\nGeneration-aspect metrics: For evaluating the quality of generation, we support five metrics\\nincluding token-level F1 score, exact match, accuracy, BLEU [ 69], and ROUGE-L [ 70]. Moreover,\\nwe support evaluating the number of tokens used in generation, to facilitate the analysis of the overall\\nprocess cost.\\nTo accommodate custom evaluation metrics, our library provides a metric template for users to\\nimplement. As our library automatically saves intermediate results of the execution, users can\\nconveniently evaluate the outcomes produced by intermediate components. For example, users might\\ncompare the number of tokens before and after the refiner runs, or the precision differences between\\nmultiple rounds of retrieval results.\\n4 Experimental Result and Discussion\\nFlashRAG can enable researchers to benchmark RAG methods, evaluate their own RAG approaches,\\nand conduct investigations within the RAG field. To demonstrate the capabilities of FlashRAG, we\\nconducted several experiments for providing reproducible benchmarks and exploration.\\nExperimental Setup. In our main experiment, we employed the latest LLAMA3-8B-instruct [ 71]\\nas the generator and the E5-base-v2 as the retriever, utilizing Wikipedia data from December 2018\\nas the retrieval corpus. The max input length of generator model is set to 4096. For each query,\\nwe retrieved five documents. For approaches not utilizing custom-defined prompts, we applied a\\nconsistent default prompt, which is shown in the appendix. Methods requiring specific settings\\nand hyperparameters are marked with asterisks in our tables, with their specific configurations\\nnoted in the appendix. All experiments are carried out on 8 NVIDIA A100 GPUs. We conducted\\nexperiments on six common datasets: Natural Questions(NQ) [ 38], TriviaQA [ 39], HotpotQA [ 52],\\n2WikiMultihopQA [ 53], PopQA [ 40] and WebQuestions [ 45]. We use exact match as the metric on\\nNQ,TriviaQA,Web Questions, and token level F1 as the metric on HotpotQA, 2WikiMultihopQA and\\nPopQA.\\nMethods. We conducted experiments on all supported RAG methods. These methods are categorized\\nbased on the RAG component they primarily focused on optimizing: AAR [ 72] aims at optimizing\\nthe retriever; LongLLMLingua [ 20], RECOMP [ 18], and Selective-Context [ 21] focus on the refiner\\nto compress input prompts; Ret-Robust [ 73] and REPLUG [ 28] focus on optimizing the generator\\nand its related decoding methods; SKR [ 10] enhances the judger that decides whether to retrieve for a\\nquery; SuRe [ 29], Self-RAG [ 33], FLARE [ 34], Iter-RetGen [ 30], and ITRG [ 31] optimize the entire\\nRAG flow, including multiple retrievals and generation processes.\\n4.1 Main results\\nThe main results of various methods are shown in Table 3. Overall, RAG methods significantly\\nimprove compared to the direct generation baseline. Standard RAG, with advanced retrievers and\\ngenerators, is a strong baseline, performing well across six datasets. AAR improves retrievers by\\ntraining the contriever model, and get comparable result to the e5 baseline on multiple datasets.\\nFor refiners, all three methods show notable improvements. Refiners perform especially well on\\nmulti-hop datasets like HotpotQA and 2WikiMultihopQA. This is likely because complex problems\\nlead to less accurate document retrieval, creating more noise and requiring refiner optimization.\\nIn generator optimization method , Ret-Robust uses the Llama2-13B model with a lora module,\\ngreatly enhancing the generator’s understanding of retrieved documents and outperforming other\\n7Table 3: The performance evaluation of RAG methods was carried out on three datasets. Optimize\\ncomponent represents the primary component optimized by the method, while flow indicates\\noptimization of the entire RAG process. Methods marked with ∗denote the use of a trained generator.\\nOptimize Pipeline NQ TriviaQA HotpotQA 2Wiki PopQA WebQAMethodcomponent type (EM) (EM) (F1) (F1) (F1) (EM)\\nNaive Generation - Sequential 22.6 55.7 28.4 33.9 21.7 18.8\\nStandard RAG - Sequential 35.1 58.8 35.3 21.0 36.7 15.7\\nAAR [72] Retriever Sequential 30.1 56.8 33.4 19.8 36.1 16.1\\nLongLLMLingua [20] Refiner Sequential 32.2 59.2 37.5 25.0 38.7 17.5\\nRECOMP-abstractive [18] Refiner Sequential 33.1 56.4 37.5 32.4 39.9 20.2\\nSelective-Context [21] Refiner Sequential 30.5 55.6 34.4 18.5 33.5 17.3\\nRet-Robust∗[73] Generator Sequential 42.9 68.2 35.8 43.4 57.2 9.1\\nSuRe [29] Flow Branching 37.1 53.2 33.4 20.6 48.1 24.2\\nREPLUG [28] Generator Branching 28.9 57.7 31.2 21.1 27.8 20.2\\nSKR [10] Judger Conditional 25.5 55.9 29.8 28.5 24.5 18.6\\nSelf-RAG∗[33] Flow Loop 36.4 38.2 29.6 25.1 32.7 21.9\\nFLARE [34] Flow Loop 22.5 55.8 28.0 33.9 20.7 20.2\\nIter-RetGen [30], ITRG [31] Flow Loop 36.8 60.1 38.3 21.6 37.9 18.2\\n1 3 5 10 15\\nNumber of Retreival Documents20.022.525.027.530.032.535.037.540.0Average Score\\nE5-base\\nBM25\\nBge-base\\nNQ TriviaQA HotpotQA 2Wiki PopQA WebQ\\nDataset0102030405060Metric ScoreTop 1\\nTop 3\\nTop 5\\nTop 10\\nTop 15\\nFigure 2: The results of standard RAG process under different number of retrieved documents and\\nretrievers. Left: Average results on six datasets using three different retrievers with varying numbers\\nof retrieved documents. Right: Individual results on six datasets using E5 as the retriever.\\ntraining-free methods. The effectiveness of optimizing the RAG process varies by dataset. On simpler\\ndatasets like NQ and TriviaQA, FLARE and Iter-RetGen are on par with or slightly below standard\\nRAG. However, on complex datasets that requiring multi-step reasoning, like HotpotQA, there are\\nsignificant improvements over the baseline. This suggests adaptive retrieval methods are more suited\\nfor complex problems, while on simpler tasks, they may incur higher costs with only modest benefits.\\n4.2 Impact of Retrieval on RAG\\nIn RAG process, the retriever is a crucial component that significantly impacts the results. The\\nquantity and quality of input retrieved documents determine the final answer. However, due to\\nconsiderations such as cost, existing research works often employs a fixed retriever and a fixed\\nnumber of retrieved documents, neglecting exploration in this area. To thoroughly investigate the\\ninfluence of the retrieval process on overall RAG results, we conducted a series of experiments.\\nIn Figure 2, we present the results for varying numbers of retrieved documents. As shown in the left\\npart of Figure 2, the overall performance is optimal when the number of retrieved documents is 3 or\\n5. Both an excessive and insufficient number of retrieved documents lead to a significant decrease in\\nperformance, with a drop of up to 40%. This trend is consistent across different retrievers, including\\nboth dense and sparse retrieval methods. Additionally, we observe that when the number of retrieved\\ndocuments is large, the results of the three different quality retrievers converge. In contrast, for the\\ntop1 results, there is a substantial gap between dense methods (E5, Bge) and BM25, indicating that\\nthe fewer documents retrieved, the greater the impact of the retriever’s quality on the final result.\\n8In the right part of Figure 2, we plot the impact of the number of retrieved documents on different\\ndatasets. It can be seen that on most datasets, using top3 or top5 retrieved results yields the best\\nperformance, suggesting that this may represent a good balance between the quality of retrieved\\ndocuments and noise.\\n5 Limitations\\nOur toolkit currently has some limitations, which we plan to gradually improve in the future.\\n(1) Although we strive to encompass many representative RAG methods, due to time and cost\\nconsiderations, we have not included all existing RAG works. This may require contributions from\\nthe open-source community in the future. (2) Our toolkit lacks support for training RAG-related\\ncomponents. We considered training during the initial design, but given the diversity of training\\nmethods and the presence of many repositories specifically dedicated to the training of retrievers and\\ngenerators, we did not include this part. In the future, we may add some helping scripts to provide\\nsome assistance for researchers’ training needs.\\n6 Conclusion\\nTo address the challenges researchers face in replicating studies and the high development costs\\nassociated with research in the RAG domain, we introduce a modular RAG toolkit. Our toolkit\\nincludes comprehensive RAG benchmark datasets, implementations of advanced RAG methods,\\nand code for pre-processing corpus and multiple evaluation metrics. It enable researchers to easily\\nreproduce existing RAG methods, develop new algorithms, and focus on optimizing their research.\\nReferences\\n[1]Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\\nMillican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\\net al. Improving language models by retrieving from trillions of tokens. In International\\nConference on Machine Learning , pages 2206–2240. PMLR, 2022.\\n[2]Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. In International Conference on Machine\\nLearning . JMLR.org, 2020.\\n[3]Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy\\nLovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu, and Pascale Fung. A\\nmultitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and\\ninteractivity, 2023.\\n[4] Harrison Chase. LangChain, October 2022.\\n[5] Jerry Liu. LlamaIndex, November 2022.\\n[6]Malte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano Pippi, Mayank\\nJobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic, Thomas Stadelmann, Tanay\\nSoni, and Sebastian Lee. Haystack: the end-to-end NLP framework for pragmatic builders,\\nNovember 2019.\\n[7]Peter Izsak, Moshe Berchansky, Daniel Fleischer, and Ronen Laperdon. fastRAG: Efficient\\nRetrieval Augmentation and Generation Framework, February 2023.\\n[8]Xiao Yu, Yunan Lu, and Zhou Yu. Localrqa: From generating data to locally training, testing,\\nand deploying retrieval-augmented qa systems, 2024.\\n[9] Jeffrey Kim Bwook Kim. AutoRAG, 2024.\\n[10] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. Self-knowledge guided retrieval augmentation\\nfor large language models. In Findings of the Association for Computational Linguistics:\\nEMNLP 2023 , pages 10303–10315, Singapore, December 2023. Association for Computational\\nLinguistics.\\n[11] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo\\nNogueira. Pyserini: A Python toolkit for reproducible information retrieval research with\\n9sparse and dense representations. In Proceedings of the 44th Annual International ACM\\nSIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021) , pages\\n2356–2362, 2021.\\n[12] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\\nInProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP) , pages 6769–6781, Online, November 2020. Association for Computational\\nLinguistics.\\n[13] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan\\nMajumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training.\\narXiv preprint arXiv:2212.03533 , 2022.\\n[14] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources\\nto advance general chinese embedding, 2023.\\n[15] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid\\nAhmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for\\ndense text retrieval. In International Conference on Learning Representations , 2021.\\n[16] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-\\nEmmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library. 2024.\\n[17] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs.\\nIEEE Transactions on Big Data , 7(3):535–547, 2019.\\n[18] Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with\\ncompression and selective augmentation. arXiv preprint arXiv:2310.04408 , 2023.\\n[19] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua:\\nCompressing prompts for accelerated inference of large language models. In Proceedings\\nof the 2023 Conference on Empirical Methods in Natural Language Processing , pages 13358–\\n13376. Association for Computational Linguistics, December 2023.\\n[20] Huiqiang Jiang, Qianhui Wu, , Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and\\nLili Qiu. LongLLMLingua: Accelerating and enhancing llms in long context scenarios via\\nprompt compression. ArXiv preprint , abs/2310.06839, 2023.\\n[21] Yucheng Li. Unlocking context constraints of llms: Enhancing context efficiency of llms with\\nself-information-based content filtering. arXiv preprint arXiv:2304.12102 , 2023.\\n[22] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\\nJoseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large\\nlanguage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th\\nSymposium on Operating Systems Principles , 2023.\\n[23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\\n[24] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-\\nart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods\\nin Natural Language Processing: System Demonstrations , pages 38–45, Online, October 2020.\\nAssociation for Computational Linguistics.\\n[25] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\\nInternational Conference on Learning Representations .\\n[26] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models\\nfor open domain question answering. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty,\\neditors, Proceedings of the 16th Conference of the European Chapter of the Association for\\nComputational Linguistics: Main Volume , pages 874–880, Online, April 2021. Association for\\nComputational Linguistics.\\n10[27] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei\\nSun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large\\nlanguage models: A survey, 2024.\\n[28] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\\nZettlemoyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models, 2023.\\n[29] Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-\\nWoo Ha, and Jinwoo Shin. Sure: Summarizing retrievals using answer candidates for open-\\ndomain QA of LLMs. In The Twelfth International Conference on Learning Representations ,\\n2024.\\n[30] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.\\nEnhancing retrieval-augmented large language models with iterative retrieval-generation synergy.\\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for\\nComputational Linguistics: EMNLP 2023 , pages 9248–9274, Singapore, December 2023.\\nAssociation for Computational Linguistics.\\n[31] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. Retrieval-generation\\nsynergy augmented large language models, 2023.\\n[32] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring\\nand narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and\\nKalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 ,\\npages 5687–5711, Singapore, December 2023. Association for Computational Linguistics.\\n[33] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG:\\nLearning to retrieve, generate, and critique through self-reflection. In The Twelfth International\\nConference on Learning Representations , 2024.\\n[34] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming\\nYang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint\\narXiv:2305.06983 , 2023.\\n[35] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the\\nInternational Conference on Learning Representations (ICLR) , 2021.\\n[36] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\\nSteinhardt. Aligning ai with shared human values. Proceedings of the International Conference\\non Learning Representations (ICLR) , 2021.\\n[37] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\\nelectricity? a new dataset for open book question answering. In EMNLP , 2018.\\n[38] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\\nPetrov. Natural questions: A benchmark for question answering research. Transactions of the\\nAssociation for Computational Linguistics , 7:452–466, 2019.\\n[39] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale\\ndistantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-\\nYen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1601–1611, Vancouver, Canada, July 2017.\\nAssociation for Computational Linguistics.\\n[40] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel\\nKhashabi. When not to trust language models: Investigating effectiveness and limitations\\nof parametric and non-parametric memories. arXiv preprint , 2022.\\n[41] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+\\nquestions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras,\\neditors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language\\nProcessing , pages 2383–2392, Austin, Texas, November 2016. Association for Computational\\nLinguistics.\\n[42] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and\\nLi Deng. MS MARCO: A human-generated MAchine reading COmprehension dataset, 2017.\\n11[43] Tomáš Koˇ ciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann,\\nGábor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge.\\nTransactions of the Association for Computational Linguistics , TBD:TBD, 2018.\\n[44] Yi Yang, Wen-tau Yih, and Christopher Meek. WikiQA: A challenge dataset for open-domain\\nquestion answering. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings\\nof the 2015 Conference on Empirical Methods in Natural Language Processing , pages 2013–\\n2018, Lisbon, Portugal, September 2015. Association for Computational Linguistics.\\n[45] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase\\nfrom question-answer pairs. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen\\nLivescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing , pages 1533–1544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics.\\n[46] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. AmbigQA: Answering\\nambiguous open-domain questions. In EMNLP , 2020.\\n[47] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa:\\nCommonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent\\nNg, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods\\nin Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pages 4463–4473, Hong Kong, China, November\\n2019. Association for Computational Linguistics.\\n[48] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A\\nquestion answering challenge targeting commonsense knowledge. In Jill Burstein, Christy\\nDoran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies,\\nVolume 1 (Long and Short Papers) , pages 4149–4158, Minneapolis, Minnesota, June 2019.\\nAssociation for Computational Linguistics.\\n[49] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In\\nNAACL , 2019.\\n[50] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning\\nabout physical commonsense in natural language. In AAAI Conference on Artificial Intelligence ,\\n2019.\\n[51] Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark.\\nHow much coffee was consumed during emnlp 2019? fermi problems: A new reasoning\\nchallenge for ai. arXiv preprint arXiv:2110.14207 , 2021.\\n[52] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\\nanswering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors,\\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing ,\\npages 2369–2380, Brussels, Belgium, October-November 2018. Association for Computational\\nLinguistics.\\n[53] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a\\nmulti-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the\\n28th International Conference on Computational Linguistics , pages 6609–6625, Barcelona,\\nSpain (Online), December 2020. International Committee on Computational Linguistics.\\n[54] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue:\\nMultihop questions via single-hop question composition. Transactions of the Association for\\nComputational Linguistics , 2022.\\n[55] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions\\nmeet long-form answers. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,\\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing ,\\npages 8273–8288, Abu Dhabi, United Arab Emirates, December 2022. Association for\\nComputational Linguistics.\\n[56] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Anna Korhonen, David Traum, and Lluís Màrquez, editors,\\n12Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages\\n3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics.\\n[57] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic\\nhuman falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,\\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 3214–3252, Dublin, Ireland, May 2022. Association for\\nComputational Linguistics.\\n[58] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can\\na machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the\\nAssociation for Computational Linguistics , 2019.\\n[59] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\nand Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning\\nchallenge. CoRR , abs/1803.05457, 2018.\\n[60] Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal,\\nMarc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. Robust disambiguation of\\nnamed entities in text. In Regina Barzilay and Mark Johnson, editors, Proceedings of the 2011\\nConference on Empirical Methods in Natural Language Processing , pages 782–792, Edinburgh,\\nScotland, UK., July 2011. Association for Computational Linguistics.\\n[61] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao,\\nJames Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim\\nRocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks.\\nInProceedings of the 2021 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies , pages 2523–2544, Online, June\\n2021. Association for Computational Linguistics.\\n[62] Simone Tedeschi, Simone Conia, Francesco Cecconi, and Roberto Navigli. Named Entity\\nRecognition for Entity Linking: What works and what’s next. In Findings of the Association for\\nComputational Linguistics: EMNLP 2021 , pages 2584–2596, Punta Cana, Dominican Republic,\\nNovember 2021. Association for Computational Linguistics.\\n[63] Hady ElSahar, Pavlos V ougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare,\\nFrédérique Laforest, and Elena Simperl. T-rex: A large scale alignment of natural language with\\nknowledge base triples. In Proceedings of the Eleventh International Conference on Language\\nResources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018. , 2018.\\n[64] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction\\nvia reading comprehension. In Roger Levy and Lucia Specia, editors, Proceedings of the 21st\\nConference on Computational Natural Language Learning (CoNLL 2017) , pages 333–342,\\nVancouver, Canada, August 2017. Association for Computational Linguistics.\\n[65] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERification. In NAACL-HLT , 2018.\\n[66] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston.\\nWizard of Wikipedia: Knowledge-powered conversational agents. In International Conference\\non Learning Representations , 2019.\\n[67] Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham\\nNeubig. Wikiasp: A dataset for multi-domain aspect-based summarization. Transactions of the\\nAssociation for Computational Linguistics (TACL) , 2020.\\n[68] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer\\nopen-domain questions. In Association for Computational Linguistics (ACL) , 2017.\\n[69] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for\\nComputational Linguistics , ACL ’02, page 311–318, USA, 2002. Association for Computational\\nLinguistics.\\n[70] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text\\nSummarization Branches Out , pages 74–81, Barcelona, Spain, July 2004. Association for\\nComputational Linguistics.\\n[71] AI@Meta. Llama 3 model card. 2024.\\n13[72] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. Augmentation-adapted retriever improves\\ngeneralization of language models as generic plug-in. In Anna Rogers, Jordan Boyd-Graber,\\nand Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pages 2421–2436, Toronto, Canada, July\\n2023. Association for Computational Linguistics.\\n[73] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented\\nlanguage models robust to irrelevant context, 2023.\\n14'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1791f8d18b62294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T03:36:46.592523Z",
     "start_time": "2024-07-15T03:36:46.587397Z"
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=500,\n",
    "  chunk_overlap=50\n",
    ")\n",
    "texts = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a15ab9ef2c90bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T03:44:19.429010Z",
     "start_time": "2024-07-15T03:44:19.423191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FlashRAG: A Modular Toolkit for Efficient\\nRetrieval-Augmented Generation Research\\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou∗\\nGaoling School of Artificial Intelligence\\nRenmin University of China\\n{jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com\\nAbstract\\nWith the advent of Large Language Models (LLMs), the potential of Retrieval\\nAugmented Generation (RAG) techniques have garnered considerable research'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
