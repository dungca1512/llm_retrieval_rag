{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "\n",
      "Long-context LLMs.\n",
      "\n",
      "Chunk 2:\n",
      "There has long been efforts for enabling LLMs to handle long contexts \n",
      "(Guo et al., 2022; Beltagy et al., 2020; Chen et al., \n",
      "2023b).\n",
      "\n",
      "Chunk 3:\n",
      "While recent LLMs like Gemini-1.5 (Reid\n",
      "et al., 2024), GPT-4 (Achiam et al., 2023), Claude3 (Anthropic, 2024) achieve significantly larger\n",
      "context window size, long-context prompting is\n",
      "still expensive due to the quadratic computation\n",
      "cost of transformers regarding to the input token\n",
      "numbers.\n",
      "\n",
      "Chunk 4:\n",
      "Recent work proposes methods to reduce\n",
      "cost by prompt compression (Jiang et al., 2023),\n",
      "model distillation (Hsieh et al., 2023), or LLM cascading (Chen et al., 2023a).\n",
      "\n",
      "\n",
      "\n",
      "Chunk 5:\n",
      "Retrieval-augmented generation.\n",
      "\n",
      "Chunk 6:\n",
      "Augmenting\n",
      "LLMs with relevant information retrieved from\n",
      "various sources (Lewis et al., 2020), i.e., RAG,\n",
      "has been successful in complementing LLMs with\n",
      "external knowledge.\n",
      "\n",
      "Chunk 7:\n",
      "RAG achieves good performance on various of tasks like language modeling\n",
      "(Khandelwal et al., 2019; Shi et al., 2023) and QA\n",
      "(Guu et al., 2020; Izacard and Grave, 2020), with\n",
      "a significantly lower computation cost (Borgeaud\n",
      "et al., 2022).\n",
      "\n",
      "Chunk 8:\n",
      "Related to but different from our work,\n",
      "recently works augment RAG with correction (Yan\n",
      "et al., 2024), critique (Asai et al., 2023), or verification (Li et al., 2023) to improve retrieval quality\n",
      "on knowledge-intensive tasks.\n",
      "\n",
      "\n",
      "Chunk 9:\n",
      "Long-context evaluation.\n",
      "\n",
      "Chunk 10:\n",
      "Evaluating long-context\n",
      "models is challenging due to the difficulty in\n",
      "collecting and analyzing long texts.\n",
      "\n",
      "Chunk 11:\n",
      "Recent researchers propose both synthetic tests like needlein-a-haystack (Greg Kamradt, 2023), Ruler (Hsieh\n",
      "et al., 2024), or Counting Stars (Song et al., 2024),\n",
      "and real datasets including LongBench (Bai et al.,\n",
      "2023), ∞Bench (Zhang et al., 2024), L-Eval (An\n",
      "et al., 2023), and others (Shaham et al., 2022; Yuan\n",
      "et al., 2024; Maharana et al., 2024). Evaluating\n",
      "on these datasets, recent works study the performance degradation over various context lengths\n",
      "(Levy et al., 2024; Hsieh et al., 2024), the lostin-the-middle phenomenon (Liu et al., 2024), and\n",
      "explore solutions (Kuratov et al., 2024).\n",
      "\n",
      "Chunk 12:\n",
      "Related\n",
      "to our work, Xu et al. (2023) compare RAG and\n",
      "long-context prompting and find that long-context\n",
      "models still lags behind RAG.\n",
      "\n",
      "Chunk 13:\n",
      "This is different\n",
      "from our findings, possibly due to consideration of\n",
      "stronger LLMs and longer contexts in our work.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the spaCy model for sentence segmentation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Semantic splitting based on sentence boundaries and similarity\n",
    "def semantic_splitting(text, threshold=0.3):\n",
    "    # Parse the document\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]  # Extract sentences\n",
    "\n",
    "    # Vectorize the sentences for similarity checking\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Calculate pairwise cosine similarity between sentences\n",
    "    similarities = cosine_similarity(vectors)\n",
    "\n",
    "    # Initialize chunks with the first sentence\n",
    "    chunks = [[sentences[0]]]\n",
    "\n",
    "    # Group sentences into chunks based on similarity threshold\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim_score = similarities[i-1, i]\n",
    "\n",
    "        if sim_score >= threshold:\n",
    "            # If the similarity is above the threshold, add to the current chunk\n",
    "            chunks[-1].append(sentences[i])\n",
    "        else:\n",
    "            # Start a new chunk\n",
    "            chunks.append([sentences[i]])\n",
    "\n",
    "    # Join the sentences in each chunk to form coherent paragraphs\n",
    "    return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "Long-context LLMs. There has long been efforts for enabling LLMs to handle long contexts \n",
    "(Guo et al., 2022; Beltagy et al., 2020; Chen et al., \n",
    "2023b). While recent LLMs like Gemini-1.5 (Reid\n",
    "et al., 2024), GPT-4 (Achiam et al., 2023), Claude3 (Anthropic, 2024) achieve significantly larger\n",
    "context window size, long-context prompting is\n",
    "still expensive due to the quadratic computation\n",
    "cost of transformers regarding to the input token\n",
    "numbers. Recent work proposes methods to reduce\n",
    "cost by prompt compression (Jiang et al., 2023),\n",
    "model distillation (Hsieh et al., 2023), or LLM cascading (Chen et al., 2023a).\n",
    "\n",
    "Retrieval-augmented generation. Augmenting\n",
    "LLMs with relevant information retrieved from\n",
    "various sources (Lewis et al., 2020), i.e., RAG,\n",
    "has been successful in complementing LLMs with\n",
    "external knowledge. RAG achieves good performance on various of tasks like language modeling\n",
    "(Khandelwal et al., 2019; Shi et al., 2023) and QA\n",
    "(Guu et al., 2020; Izacard and Grave, 2020), with\n",
    "a significantly lower computation cost (Borgeaud\n",
    "et al., 2022). Related to but different from our work,\n",
    "recently works augment RAG with correction (Yan\n",
    "et al., 2024), critique (Asai et al., 2023), or verification (Li et al., 2023) to improve retrieval quality\n",
    "on knowledge-intensive tasks.\n",
    "Long-context evaluation. Evaluating long-context\n",
    "models is challenging due to the difficulty in\n",
    "collecting and analyzing long texts. Recent researchers propose both synthetic tests like needlein-a-haystack (Greg Kamradt, 2023), Ruler (Hsieh\n",
    "et al., 2024), or Counting Stars (Song et al., 2024),\n",
    "and real datasets including LongBench (Bai et al.,\n",
    "2023), ∞Bench (Zhang et al., 2024), L-Eval (An\n",
    "et al., 2023), and others (Shaham et al., 2022; Yuan\n",
    "et al., 2024; Maharana et al., 2024). Evaluating\n",
    "on these datasets, recent works study the performance degradation over various context lengths\n",
    "(Levy et al., 2024; Hsieh et al., 2024), the lostin-the-middle phenomenon (Liu et al., 2024), and\n",
    "explore solutions (Kuratov et al., 2024). Related\n",
    "to our work, Xu et al. (2023) compare RAG and\n",
    "long-context prompting and find that long-context\n",
    "models still lags behind RAG. This is different\n",
    "from our findings, possibly due to consideration of\n",
    "stronger LLMs and longer contexts in our work.\n",
    "\"\"\"\n",
    "\n",
    "# Perform semantic splitting\n",
    "semantic_chunks = semantic_splitting(text)\n",
    "\n",
    "# Print the chunks\n",
    "for idx, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"Chunk {idx+1}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dungca/anaconda3/envs/llm_retrieval_rag/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'IPython.utils.traitlets' has no attribute 'Unicode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b99a8518d0b0>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_extraction_chain_pydantic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/llm_retrieval_rag/lib/python3.9/site-packages/google/colab/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_import_magics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_installation_commands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_interactive_table_hint_button\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_reprs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_shell_customizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/llm_retrieval_rag/lib/python3.9/site-packages/google/colab/_interactive_table_hint_button.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muuid\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_uuid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweakref\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_weakref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_table\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_data_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/llm_retrieval_rag/lib/python3.9/site-packages/google/colab/data_table.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_JavascriptModuleFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseFormatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m   \u001b[0mformat_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_traitlets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_JAVASCRIPT_MODULE_MIME_TYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m   \u001b[0mprint_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_traitlets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_repr_javascript_module_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/llm_retrieval_rag/lib/python3.9/site-packages/google/colab/data_table.py\u001b[0m in \u001b[0;36m_JavascriptModuleFormatter\u001b[0;34m()\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_JavascriptModuleFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseFormatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m   \u001b[0mformat_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_traitlets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_JAVASCRIPT_MODULE_MIME_TYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m   \u001b[0mprint_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_traitlets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_repr_javascript_module_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'IPython.utils.traitlets' has no attribute 'Unicode'"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import uuid\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "from typing import Optional\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from dotenv import load_dotenv\n",
    "from google.colab import userdata\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AgenticChunker:\n",
    "    def __init__(self, openai_api_key=None):\n",
    "        self.chunks = {}\n",
    "        self.id_truncate_limit = 5\n",
    "\n",
    "        # Whether or not to update/refine summaries and titles as you get new information\n",
    "        self.generate_new_metadata_ind = True\n",
    "        self.print_logging = True\n",
    "\n",
    "        if openai_api_key is None:\n",
    "            openai_api_key = userdata.get(\"open_ai_key\")\n",
    "\n",
    "        if openai_api_key is None:\n",
    "            raise ValueError(\"API key is not provided and not found in environment variables\")\n",
    "\n",
    "        self.llm = ChatOpenAI(model='gpt-4-1106-preview', openai_api_key=openai_api_key, temperature=0)\n",
    "\n",
    "    # Người dùng cung cấp danh sách các propositions (mệnh đề) để thêm vào hệ thống.\n",
    "    def add_propositions(self, propositions):\n",
    "        for proposition in propositions:\n",
    "            self.add_proposition(proposition)\n",
    "\n",
    "    def add_proposition(self, proposition):\n",
    "        if self.print_logging:\n",
    "            print (f\"\\nAdding: '{proposition}'\")\n",
    "\n",
    "        # If it's your first chunk, just make a new chunk and don't check for others\n",
    "        if len(self.chunks) == 0:\n",
    "            if self.print_logging:\n",
    "                print (\"No chunks, creating a new one\")\n",
    "            self._create_new_chunk(proposition)\n",
    "            return\n",
    "\n",
    "        chunk_id = self._find_relevant_chunk(proposition)\n",
    "\n",
    "        # If a chunk was found then add the proposition to it\n",
    "        if chunk_id:\n",
    "            if self.print_logging:\n",
    "                print (f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
    "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
    "            return\n",
    "        else:\n",
    "            if self.print_logging:\n",
    "                print (\"No chunks found\")\n",
    "            # If a chunk wasn't found, then create a new one\n",
    "            self._create_new_chunk(proposition)\n",
    "\n",
    "\n",
    "    def add_proposition_to_chunk(self, chunk_id, proposition):\n",
    "        # Add then\n",
    "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
    "\n",
    "        # Then grab a new summary\n",
    "        if self.generate_new_metadata_ind:\n",
    "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
    "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
    "\n",
    "\n",
    "    # Cập nhật summary khi thêm Chunk mới\n",
    "    def _update_chunk_summary(self, chunk):\n",
    "        \"\"\"\n",
    "        If you add a new proposition to a chunk, you may want to update the summary or else they could get stale\n",
    "        \"\"\"\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk and the chunks current summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the chunk new summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nCurrent chunk summary:\\n{current_summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary']\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "\n",
    "    def _update_chunk_title(self, chunk):\n",
    "        \"\"\"\n",
    "        If you add a new proposition to a chunk, you may want to update the title or else it can get stale\n",
    "        \"\"\"\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good title will say what the chunk is about.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\n",
    "\n",
    "                    Your title should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nChunk summary:\\n{current_summary}\\n\\nCurrent chunk title:\\n{current_title}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        updated_chunk_title = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary'],\n",
    "            \"current_title\" : chunk['title']\n",
    "        }).content\n",
    "\n",
    "        return updated_chunk_title\n",
    "\n",
    "    def _get_new_chunk_summary(self, proposition):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the new chunk summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": proposition\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "\n",
    "    def _get_new_chunk_title(self, summary):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good chunk title is brief but encompasses what the chunk is about\n",
    "\n",
    "                    You will be given a summary of a chunk which needs a title\n",
    "\n",
    "                    Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the title of the chunk that this summary belongs to:\\n{summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_title = runnable.invoke({\n",
    "            \"summary\": summary\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_title\n",
    "\n",
    "\n",
    "    def _create_new_chunk(self, proposition):\n",
    "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit] # I don't want long ids\n",
    "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
    "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
    "\n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id' : new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title' : new_chunk_title,\n",
    "            'summary': new_chunk_summary,\n",
    "            'chunk_index' : len(self.chunks)\n",
    "        }\n",
    "        if self.print_logging:\n",
    "            print (f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
    "\n",
    "    def get_chunk_outline(self):\n",
    "        \"\"\"\n",
    "        Get a string which represents the chunks you currently have.\n",
    "        This will be empty when you first start off\n",
    "        \"\"\"\n",
    "        chunk_outline = \"\"\n",
    "\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            single_chunk_string = f\"\"\"Chunk ID: {chunk['chunk_id']}\\nChunk Name: {chunk['title']}\\nChunk Summary: {chunk['summary']}\\n\\n\"\"\"\n",
    "\n",
    "            chunk_outline += single_chunk_string\n",
    "\n",
    "        return chunk_outline\n",
    "\n",
    "    # Kiểm tra xem có chunk nào hiện có phù hợp với proposition mới hay không\n",
    "    # bằng cách so sánh ý nghĩa của chúng.\n",
    "    def _find_relevant_chunk(self, proposition):\n",
    "        current_chunk_outline = self.get_chunk_outline()\n",
    "\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    Determine whether or not the \"Proposition\" should belong to any of the existing chunks.\n",
    "\n",
    "                    A proposition should belong to a chunk of their meaning, direction, or intention are similar.\n",
    "                    The goal is to group similar propositions and chunks.\n",
    "\n",
    "                    If you think a proposition should be joined with a chunk, return the chunk id.\n",
    "                    If you do not think an item should be joined with an existing chunk, just return \"No chunks\"\n",
    "\n",
    "                    Example:\n",
    "                    Input:\n",
    "                        - Proposition: \"Greg really likes hamburgers\"\n",
    "                        - Current Chunks:\n",
    "                            - Chunk ID: 2n4l3d\n",
    "                            - Chunk Name: Places in San Francisco\n",
    "                            - Chunk Summary: Overview of the things to do with San Francisco Places\n",
    "\n",
    "                            - Chunk ID: 93833k\n",
    "                            - Chunk Name: Food Greg likes\n",
    "                            - Chunk Summary: Lists of the food and dishes that Greg likes\n",
    "                    Output: 93833k\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\"),\n",
    "                (\"user\", \"Determine if the following statement should belong to one of the chunks outlined:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        chunk_found = runnable.invoke({\n",
    "            \"proposition\": proposition,\n",
    "            \"current_chunk_outline\": current_chunk_outline\n",
    "        }).content\n",
    "\n",
    "        # Pydantic data class\n",
    "        class ChunkID(BaseModel):\n",
    "            \"\"\"Extracting the chunk id\"\"\"\n",
    "            chunk_id: Optional[str]\n",
    "\n",
    "        # Extraction to catch-all LLM responses. This is a bandaid\n",
    "        extraction_chain = create_extraction_chain_pydantic(pydantic_schema=ChunkID, llm=self.llm)\n",
    "        extraction_found = extraction_chain.run(chunk_found)\n",
    "        if extraction_found:\n",
    "            chunk_found = extraction_found[0].chunk_id\n",
    "\n",
    "        # If you got a response that isn't the chunk id limit, chances are it's a bad response or it found nothing\n",
    "        # So return nothing\n",
    "        if len(chunk_found) != self.id_truncate_limit:\n",
    "            return None\n",
    "\n",
    "        return chunk_found\n",
    "\n",
    "    def get_chunks(self, get_type='dict'):\n",
    "        \"\"\"\n",
    "        This function returns the chunks in the format specified by the 'get_type' parameter.\n",
    "        If 'get_type' is 'dict', it returns the chunks as a dictionary.\n",
    "        If 'get_type' is 'list_of_strings', it returns the chunks as a list of strings, where each string is a proposition in the chunk.\n",
    "        \"\"\"\n",
    "        if get_type == 'dict':\n",
    "            return self.chunks\n",
    "        if get_type == 'list_of_strings':\n",
    "            chunks = []\n",
    "            for chunk_id, chunk in self.chunks.items():\n",
    "                chunks.append(\" \".join([x for x in chunk['propositions']]))\n",
    "            return chunks\n",
    "\n",
    "    def pretty_print_chunks(self):\n",
    "        print (f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
    "            print(f\"Chunk ID: {chunk_id}\")\n",
    "            print(f\"Summary: {chunk['summary']}\")\n",
    "            print(f\"Propositions:\")\n",
    "            for prop in chunk['propositions']:\n",
    "                print(f\"    -{prop}\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    def pretty_print_chunk_outline(self):\n",
    "        print (\"Chunk Outline\\n\")\n",
    "        print(self.get_chunk_outline())\n",
    "\n",
    "ac = AgenticChunker()\n",
    "\n",
    "## Comment and uncomment the propositions to your hearts content\n",
    "propositions = [\n",
    "    \"The only way to be successful is to get up at the crack of dawn – or so the story goes.\",\n",
    "    \"But early rising productivity is not a one-size-fits-all situation, Bryan Lufkin finds.\",\n",
    "    \"Being successful means waking up early – or so we’re constantly told.\"\n",
    "]\n",
    "\n",
    "ac.add_propositions(propositions)\n",
    "ac.pretty_print_chunks()\n",
    "ac.pretty_print_chunk_outline()\n",
    "print (ac.get_chunks(get_type='list_of_strings'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_retrieval_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
