{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "\n",
      "Long-context LLMs.\n",
      "\n",
      "Chunk 2:\n",
      "There has long been efforts for enabling LLMs to handle long contexts \n",
      "(Guo et al., 2022; Beltagy et al., 2020; Chen et al., \n",
      "2023b).\n",
      "\n",
      "Chunk 3:\n",
      "While recent LLMs like Gemini-1.5 (Reid\n",
      "et al., 2024), GPT-4 (Achiam et al., 2023), Claude3 (Anthropic, 2024) achieve significantly larger\n",
      "context window size, long-context prompting is\n",
      "still expensive due to the quadratic computation\n",
      "cost of transformers regarding to the input token\n",
      "numbers.\n",
      "\n",
      "Chunk 4:\n",
      "Recent work proposes methods to reduce\n",
      "cost by prompt compression (Jiang et al., 2023),\n",
      "model distillation (Hsieh et al., 2023), or LLM cascading (Chen et al., 2023a).\n",
      "\n",
      "\n",
      "\n",
      "Chunk 5:\n",
      "Retrieval-augmented generation.\n",
      "\n",
      "Chunk 6:\n",
      "Augmenting\n",
      "LLMs with relevant information retrieved from\n",
      "various sources (Lewis et al., 2020), i.e., RAG,\n",
      "has been successful in complementing LLMs with\n",
      "external knowledge.\n",
      "\n",
      "Chunk 7:\n",
      "RAG achieves good performance on various of tasks like language modeling\n",
      "(Khandelwal et al., 2019; Shi et al., 2023) and QA\n",
      "(Guu et al., 2020; Izacard and Grave, 2020), with\n",
      "a significantly lower computation cost (Borgeaud\n",
      "et al., 2022).\n",
      "\n",
      "Chunk 8:\n",
      "Related to but different from our work,\n",
      "recently works augment RAG with correction (Yan\n",
      "et al., 2024), critique (Asai et al., 2023), or verification (Li et al., 2023) to improve retrieval quality\n",
      "on knowledge-intensive tasks.\n",
      "\n",
      "\n",
      "Chunk 9:\n",
      "Long-context evaluation.\n",
      "\n",
      "Chunk 10:\n",
      "Evaluating long-context\n",
      "models is challenging due to the difficulty in\n",
      "collecting and analyzing long texts.\n",
      "\n",
      "Chunk 11:\n",
      "Recent researchers propose both synthetic tests like needlein-a-haystack (Greg Kamradt, 2023), Ruler (Hsieh\n",
      "et al., 2024), or Counting Stars (Song et al., 2024),\n",
      "and real datasets including LongBench (Bai et al.,\n",
      "2023), ∞Bench (Zhang et al., 2024), L-Eval (An\n",
      "et al., 2023), and others (Shaham et al., 2022; Yuan\n",
      "et al., 2024; Maharana et al., 2024). Evaluating\n",
      "on these datasets, recent works study the performance degradation over various context lengths\n",
      "(Levy et al., 2024; Hsieh et al., 2024), the lostin-the-middle phenomenon (Liu et al., 2024), and\n",
      "explore solutions (Kuratov et al., 2024).\n",
      "\n",
      "Chunk 12:\n",
      "Related\n",
      "to our work, Xu et al. (2023) compare RAG and\n",
      "long-context prompting and find that long-context\n",
      "models still lags behind RAG.\n",
      "\n",
      "Chunk 13:\n",
      "This is different\n",
      "from our findings, possibly due to consideration of\n",
      "stronger LLMs and longer contexts in our work.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the spaCy model for sentence segmentation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Semantic splitting based on sentence boundaries and similarity\n",
    "def semantic_splitting(text, threshold=0.3):\n",
    "    # Parse the document\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]  # Extract sentences\n",
    "\n",
    "    # Vectorize the sentences for similarity checking\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Calculate pairwise cosine similarity between sentences\n",
    "    similarities = cosine_similarity(vectors)\n",
    "\n",
    "    # Initialize chunks with the first sentence\n",
    "    chunks = [[sentences[0]]]\n",
    "\n",
    "    # Group sentences into chunks based on similarity threshold\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim_score = similarities[i-1, i]\n",
    "\n",
    "        if sim_score >= threshold:\n",
    "            # If the similarity is above the threshold, add to the current chunk\n",
    "            chunks[-1].append(sentences[i])\n",
    "        else:\n",
    "            # Start a new chunk\n",
    "            chunks.append([sentences[i]])\n",
    "\n",
    "    # Join the sentences in each chunk to form coherent paragraphs\n",
    "    return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "Long-context LLMs. There has long been efforts for enabling LLMs to handle long contexts \n",
    "(Guo et al., 2022; Beltagy et al., 2020; Chen et al., \n",
    "2023b). While recent LLMs like Gemini-1.5 (Reid\n",
    "et al., 2024), GPT-4 (Achiam et al., 2023), Claude3 (Anthropic, 2024) achieve significantly larger\n",
    "context window size, long-context prompting is\n",
    "still expensive due to the quadratic computation\n",
    "cost of transformers regarding to the input token\n",
    "numbers. Recent work proposes methods to reduce\n",
    "cost by prompt compression (Jiang et al., 2023),\n",
    "model distillation (Hsieh et al., 2023), or LLM cascading (Chen et al., 2023a).\n",
    "\n",
    "Retrieval-augmented generation. Augmenting\n",
    "LLMs with relevant information retrieved from\n",
    "various sources (Lewis et al., 2020), i.e., RAG,\n",
    "has been successful in complementing LLMs with\n",
    "external knowledge. RAG achieves good performance on various of tasks like language modeling\n",
    "(Khandelwal et al., 2019; Shi et al., 2023) and QA\n",
    "(Guu et al., 2020; Izacard and Grave, 2020), with\n",
    "a significantly lower computation cost (Borgeaud\n",
    "et al., 2022). Related to but different from our work,\n",
    "recently works augment RAG with correction (Yan\n",
    "et al., 2024), critique (Asai et al., 2023), or verification (Li et al., 2023) to improve retrieval quality\n",
    "on knowledge-intensive tasks.\n",
    "Long-context evaluation. Evaluating long-context\n",
    "models is challenging due to the difficulty in\n",
    "collecting and analyzing long texts. Recent researchers propose both synthetic tests like needlein-a-haystack (Greg Kamradt, 2023), Ruler (Hsieh\n",
    "et al., 2024), or Counting Stars (Song et al., 2024),\n",
    "and real datasets including LongBench (Bai et al.,\n",
    "2023), ∞Bench (Zhang et al., 2024), L-Eval (An\n",
    "et al., 2023), and others (Shaham et al., 2022; Yuan\n",
    "et al., 2024; Maharana et al., 2024). Evaluating\n",
    "on these datasets, recent works study the performance degradation over various context lengths\n",
    "(Levy et al., 2024; Hsieh et al., 2024), the lostin-the-middle phenomenon (Liu et al., 2024), and\n",
    "explore solutions (Kuratov et al., 2024). Related\n",
    "to our work, Xu et al. (2023) compare RAG and\n",
    "long-context prompting and find that long-context\n",
    "models still lags behind RAG. This is different\n",
    "from our findings, possibly due to consideration of\n",
    "stronger LLMs and longer contexts in our work.\n",
    "\"\"\"\n",
    "\n",
    "# Perform semantic splitting\n",
    "semantic_chunks = semantic_splitting(text)\n",
    "\n",
    "# Print the chunks\n",
    "for idx, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"Chunk {idx+1}:\\n{chunk}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_retrieval_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
